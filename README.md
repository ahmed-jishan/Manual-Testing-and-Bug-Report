
# ğŸ§ª Manual Testing in Software Development

Manual Testing in Software Development
Manual testing is a fundamental process in software quality assurance, where testers perform test cases manually without relying on automated testing tools or scripts. This approach requires human interaction and observation to evaluate the functionality, usability, and overall quality of a software application. The goal is to ensure the software meets specified requirements and works as intended across various scenarios.

# ğŸ¦ Tested Application

## "Securebank"
Secure Bank Website is a robust web application designed to facilitate secure financial transactions and manage user accounts efficiently. The application leverages Docker to ensure consistent environments across development, testing, and production stages, simplifying deployment and scaling. 

## âœ¨ Secure Bank Features
- User Registration: Create new user accounts with secure authentication.
- User Login: Secure login functionality with session management.
- Password Management: Forgot password and reset functionalities.
- Transaction Management: Create, view, and manage financial transactions.
- Transaction History: View past transactions with filtering options.
- Responsive Design: Accessible on various devices and screen sizes.
- Security: Implements best practices for data protection and secure transactions.
  
## ğŸ”– Manual Testing Artifacts
- Test PlanğŸ“‹
- Mind MapğŸ§ 
- Test ScenariosğŸ“œ
- Test Casesâœ…
- Test Summary ReportğŸ“ˆ
- Bug ReportğŸ
- Test MetricsğŸ“Š

## ğŸ“ Test Report
The test report provides a comprehensive summary of the project's testing activities and outcomes. It outlines the test cases executed, their results, the types of testing performed, and the identified issues requiring resolution.

### Test Cases Execution Summary
Test cases are a critical component of the project, serving as the foundation for evaluating the system's functionality, usability, and overall quality. For the Registration and Login features, all planned test cases were executed successfully within the scheduled timeframe.
- Total Test Cases Executed: 32
  - Passed: 21
  - Failed: 11
    
The testing process adhered to a structured approach, ensuring thorough coverage across various aspects of the system. The following sequence of testing was maintained:
- Browser Compatibility Testing
Ensured the application works seamlessly across different browsers (e.g., Chrome, Firefox, Edge).

- UI Testing
Verified the interface's layout, design consistency, and responsiveness.

- Functional Testing
Assessed the core functionalities of Registration and Login to confirm they perform as expected.

- Usability Testing
Evaluated the system's user-friendliness and ease of use.
### ğŸœ Bug Report
The bug report consolidates all issues identified during the execution of test cases. Specifically, the 9 failed test cases were documented and communicated to the development team for resolution. These failures are critical to address to ensure the robustness and reliability of the system before deployment.
## ğŸ§® Test Metrics

Test metrics are essential for evaluating the effectiveness and efficiency of the testing process. These metrics provide key insights into the quality of the software being tested, helping teams assess the overall performance and identify areas for improvement. The following test metrics are crucial in assessing the success of manual testing efforts:

- Percentage of Test Cases Executed
This metric represents the proportion of test cases that have been executed in comparison to the total planned test cases. A higher percentage indicates good progress in the testing process and suggests that the team is efficiently testing the system.

Formula:

```bash
  Percentage of Test Cases Executed=(Test Cases Executed / Total Test Cases)Ã—100
```

- Percentage of Test Cases Not Executed
This metric tracks the percentage of test cases that were not executed during the testing cycle. A low percentage of unexecuted test cases is desirable, indicating that the testing coverage is comprehensive.

Formula:

```bash
  Percentage of Test Cases Not Executed=(Test Cases not Executed / Total Test Cases)Ã—100
```

- Percentage of Test Cases Passed
This metric indicates the percentage of test cases that have passed, meaning the application behaved as expected under the defined conditions. A higher percentage of passed test cases is a sign of good functionality and stability.

Formula:
```bash
  Percentage of Test Cases Passed=(Test Cases Passed / Test Cases Executed)Ã—100
```
- Percentage of Test Cases Failed
This metric shows the percentage of test cases that failed, which can help identify defects or areas where the application is not meeting its requirements. A low percentage of failures is generally preferred.

Formula:
```bash
  Percentage of Test Cases Failed=(Test Cases Failed / Test Cases Executed)Ã—100
```
- Percentage of Test Cases Blocked
Blocked test cases refer to those that could not be executed due to issues outside of the testing scope (e.g., environment setup problems, dependencies on other features, etc.). Tracking this metric helps identify roadblocks in the testing process.

Formula:
```bash
  Percentage of Test Cases Blocked=(Test Cases Blocked / Test Cases Executed)Ã—100
```
- Defect Density
Defect density is a measure of the number of defects found per unit of code (usually per 1,000 lines of code or function points). This metric helps assess the quality of the code base and the thoroughness of the testing process.

Formula:
```bash
  Defect Density=(Total Defects / Size of the Code (in KLOC or Function Points))
```
- Defect Removal Efficiency (DRE)
DRE measures the effectiveness of the testing process by comparing the number of defects detected before release (during testing) with the total number of defects found (including those found post-release). A higher DRE indicates that the testing process is highly effective in catching defects before they reach the customer.

Formula:
```bash
  DRE=(Defects Found During Testing / Defects Found During Testing + Defects Found After Release)Ã—100
```
- Defect Leakage
Defect leakage refers to defects that were not identified during testing but were discovered by the end-users after release. Minimizing defect leakage is critical for maintaining software quality and customer satisfaction.

Formula:
```bash
  Defect Leakage=(Defects Found After Release / Total Defects Found)Ã—100
```
- Defect Rejection Ratio
This metric measures the percentage of defects reported by testers that are rejected by the development team due to being invalid or irrelevant. A high defect rejection ratio might indicate misunderstandings between the testers and developers, or that defects were incorrectly reported.

Formula:
```bash
  Defect Rejection Ratio=(Defects Rejected by Development Team / Total Defects Reported)Ã—100
```
- Defect Age
Defect age tracks the time elapsed between when a defect was first reported and when it was resolved. A shorter defect age is preferable, as it indicates that issues are being addressed quickly and efficiently.

Formula:
```bash
  Defect Age=Date of Defect Resolutionâˆ’Date of Defect Discovery.
```
- Customer Satisfaction
Customer satisfaction is a qualitative metric that measures how well the application meets the user's expectations. This can be gauged through surveys, feedback, and overall user experience. High customer satisfaction reflects the quality and reliability of the software product.
## ğŸ”— For a more detailed view of the project, including documentation, assets, and further updates, please refer to the following drive link
[![Drive Link](https://img.shields.io/badge/Project_Drive_Link-4CAF50?style=for-the-badge&logo=google-drive&logoColor=white)](https://docs.google.com/spreadsheets/d/1o9J7xdohn-ztnJnJ1EYHJ9f59ggMZFNu/edit?usp=sharing&ouid=113165584819262300168&rtpof=true&sd=true)


